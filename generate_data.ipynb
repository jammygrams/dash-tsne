{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='train',  remove=('headers', 'footers'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts\n",
      "data shape: (2933,)\n",
      "comp.graphics: 584\n",
      "rec.sport.baseball: 594\n",
      "talk.politics.mideast: 597\n",
      "rec.autos: 594\n",
      "sci.med: 564\n"
     ]
    }
   ],
   "source": [
    "target_names = ['comp.graphics', 'rec.sport.baseball', 'talk.politics.mideast', 'rec.autos', 'sci.med']\n",
    "target_nums = [i for i in range(20) if newsgroups_train.target_names[i] in target_names]\n",
    "\n",
    "masks = [newsgroups_train.target == i for i in target_nums]\n",
    "mask = np.array([any(tup) for tup in zip(*masks)])\n",
    "\n",
    "data = np.array(newsgroups_train.data)[mask]\n",
    "targets = np.array(newsgroups_train.target)[mask]\n",
    "\n",
    "print(f'Counts\\ndata shape: {data.shape}')\n",
    "# print(f'targets shape: {targets.shape}')\n",
    "\n",
    "for name, count in zip(target_names, [np.sum(m) for m in masks]):\n",
    "    print(f'{name}: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved in data/tfidf_labels.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def save_labels(name:string):\n",
    "    PATH = f'data/{name}_labels.csv'\n",
    "    # first row is header\n",
    "    labels = ['header'] + [newsgroups_train.target_names[i] for i in targets]\n",
    "    with open(PATH, 'w') as myfile:\n",
    "        wr = csv.writer(myfile,dialect='excel')\n",
    "        for row in labels:\n",
    "            wr.writerow([row])\n",
    "    print(f'Saved in {PATH}')\n",
    "    \n",
    "save_labels('tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Robert J.C. Kyanko (rob@rjck.UUCP) wrote:\\n> abraxis@iastate.edu writes in article <abraxis.734340159@class1.iastate.edu>:\\n> > Anyone know about the Weitek P9000 graphics chip?\\n> As far as the low-level stuff goes, it looks pretty nice.  It's got this\\n> quadrilateral fill command that requires just the four points.\\n\\nDo you have Weitek's address/phone number?  I'd like to get some information\\nabout this chip.\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robert J.C. Kyanko (rob@rjck.UUCP) wrote:\n",
      "> abraxis@iastate.edu writes in article <abraxis.734340159@class1.iastate.edu>:\n",
      "> > Anyone know about the Weitek P9000 graphics chip?\n",
      "> As far as the low-level stuff goes, it looks pretty nice.  It's got this\n",
      "> quadrilateral fill command that requires just the four points.\n",
      "\n",
      "Do you have Weitek's address/phone number?  I'd like to get some information\n",
      "about this chip.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pd.DataFrame(data)\n",
    "text.to_pickle('data/source_text.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.56 s, sys: 70.9 ms, total: 2.63 s\n",
      "Wall time: 2.67 s\n",
      "\n",
      "Vocab size: 60109\n",
      "Document vector shape: (2933, 60109)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "\n",
    "# vectorize\n",
    "bow_vec = TfidfVectorizer(max_df=0.25, min_df=0.001, ngram_range=(1, 2),\n",
    "                          sublinear_tf=True, use_idf=True)\n",
    "%time doc_bow = bow_vec.fit_transform(data)\n",
    "\n",
    "# get vocab\n",
    "vocab = bow_vec.get_feature_names()\n",
    "print('\\nVocab size:', len(vocab))\n",
    "print('Document vector shape:', doc_bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token     \t% Docs\n",
      "==============================\n",
      "these     \t19.16\n",
      "that the  \t19.77\n",
      "could     \t19.98\n",
      "for the   \t20.08\n",
      "time      \t20.12\n",
      "them      \t20.76\n",
      "people    \t21.00\n",
      "had       \t21.41\n",
      "does      \t21.55\n",
      "than      \t21.79\n",
      "com       \t21.79\n",
      "we        \t21.85\n",
      "to be     \t22.37\n",
      "were      \t22.74\n",
      "their     \t22.81\n",
      "also      \t23.05\n",
      "he        \t23.18\n",
      "only      \t23.32\n",
      "get       \t23.59\n",
      "other     \t23.63\n",
      "how       \t23.66\n",
      "been      \t23.66\n",
      "when      \t23.90\n",
      "think     \t24.21\n",
      "it is     \t24.96\n"
     ]
    }
   ],
   "source": [
    "counts = np.count_nonzero(doc_bow.toarray(), axis=0)\n",
    "percentage = 100 * counts / len(data)\n",
    "print('Token'.ljust(10), '% Docs', sep='\\t')\n",
    "print('='*30)\n",
    "for idx in np.argsort(counts)[-25:]:\n",
    "    print(vocab[idx].ljust(10), f'{percentage[idx]:.2f}', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.6 s, sys: 911 ms, total: 11.5 s\n",
      "Wall time: 7.71 s\n",
      "explained variance: 0.24124036509016744\n"
     ]
    }
   ],
   "source": [
    "# https://scikit-learn.org/stable/modules/decomposition.html\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=200, n_iter=7, random_state=42)\n",
    "%time doc_bow_svd = svd.fit_transform(doc_bow) \n",
    "\n",
    "print('explained variance:', svd.explained_variance_ratio_.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved in data/tfidf_input.csv\n"
     ]
    }
   ],
   "source": [
    "def save_vectors(vectors, name:string):\n",
    "    PATH = f\"data/{name}_input.csv\"\n",
    "    # create .csv header\n",
    "    head_vals = np.arange(vectors.shape[1])\n",
    "    header = \",\".join([item for item in head_vals.astype(str)])\n",
    "    np.savetxt(PATH, vectors, header=header, comments='', delimiter=',')\n",
    "    print(f'Saved in {PATH}')\n",
    "\n",
    "save_vectors(doc_bow_svd, 'tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save out high d object\n",
    "# TODO: TOO BIG\n",
    "# head_vals = np.arange(doc_bow_svd.shape[1])\n",
    "# header = \",\".join([item for item in head_vals.astype(str)])\n",
    "\n",
    "# np.savetxt(\"data/tfidf_input.csv\", doc_bow_svd, header=header, comments='', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nus/miniconda3/envs/tsne-vis-env/lib/python3.6/site-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "# tokenize\n",
    "def read_corpus(data, tokens_only=False):\n",
    "    for i, doc in enumerate(data):\n",
    "        if tokens_only:\n",
    "            yield gensim.utils.simple_preprocess(doc)\n",
    "        else:\n",
    "            yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(doc), [i])\n",
    "        \n",
    "corpus = list(read_corpus(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab length: 20979\n",
      "sample tokenised doc:\n",
      "TaggedDocument(['was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'saw', 'the', 'other', 'day', 'it', 'was', 'door', 'sports', 'car', 'looked', 'to', 'be', 'from', 'the', 'late', 'early', 'it', 'was', 'called', 'bricklin', 'the', 'doors', 'were', 'really', 'small', 'in', 'addition', 'the', 'front', 'bumper', 'was', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body', 'this', 'is', 'all', 'know', 'if', 'anyone', 'can', 'tellme', 'model', 'name', 'engine', 'specs', 'years', 'of', 'production', 'where', 'this', 'car', 'is', 'made', 'history', 'or', 'whatever', 'info', 'you', 'have', 'on', 'this', 'funky', 'looking', 'car', 'please', 'mail'], [0])\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=200, min_count=2, epochs=40)\n",
    "model.build_vocab(corpus)\n",
    "\n",
    "print(f'vocab length: {len(model.wv.vocab)}')\n",
    "print(f'sample tokenised doc:\\n{corpus[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 57.4 s, sys: 1.64 s, total: 59.1 s\n",
      "Wall time: 23.7 s\n"
     ]
    }
   ],
   "source": [
    "%time model.train(corpus, total_examples=model.corpus_count, epochs=model.epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.infer_vector(['only', 'you', 'can', 'prevent', 'forest', 'fires']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76.78%\n"
     ]
    }
   ],
   "source": [
    "ranks = []\n",
    "second_ranks = []\n",
    "for doc_id in range(len(corpus)):\n",
    "    inferred_vector = model.infer_vector(corpus[doc_id].words)\n",
    "    sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "    # what rank is the trained vector in order of sim to inferred vector\n",
    "    rank = [docid for docid, sim in sims].index(doc_id) \n",
    "    ranks.append(rank)\n",
    "    \n",
    "    second_ranks.append(sims[1])\n",
    "\n",
    "print(f'{100* np.sum([r==0 for r in ranks]) / len(ranks):.2f}%')\n",
    "\n",
    "# import collections\n",
    "# counter = collections.Counter(ranks)\n",
    "# sorted(counter.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document (2932): «in article qkgbuinns shelley washington edu bolson carson washington edu edward bolson writes boy this will be embarassing if it is trivial or an faq given points non coplanar how does one find the sphere that is center and radius exactly fitting those points know how to do it for circle from points but do not immediately see straightforward way to do it in have checked some geometry books graphics gems and farin but am still at loss please have mercy on me and provide the solution wouldn this require hyper sphere in space points over specifies sphere as far as can see unless that is you can prove that point exists in space that is equi distant from the points and this may not necessarily happen correct me if wrong which quite possibly am steve»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dm/m,d200,n5,w5,mc2,s0.001,t3):\n",
      "\n",
      "MOST (2657, 0.8543505668640137): «in article rb srgenprp sr hp com almanb sr hp com bob alman writes hose»\n",
      "\n",
      "SECOND-MOST (735, 0.849534273147583): «just kidding»\n",
      "\n",
      "MEDIAN (533, 0.46986547112464905): «in randall moose randall informix com randall rhea writes the royals are darkness they are the void of our time when they play shame descends upon the land like cold front from canada they are humiliation to all who have lived and all who shall ever live they are utterly and completely doomed other than that guess they re ok oh lighten up what depresses me is that they might actually finish last which believe hasn happened since their second season in never mind that gubizca is with era gardner at our main recent acquisitions lind mcreynolds jose are averaging david cone is about how he was doing in kc before joining the mets several years ago our hitting sucks and our pitching has collapsed and we ve won one game at home they ve won more games in their first ten games than last year and brian mcrae is actually batting over the mendoza line»\n",
      "\n",
      "LEAST (2085, -0.1267537921667099): «»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Document ({}): «{}»\\n'.format(doc_id, ' '.join(corpus[doc_id].words)))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('SECOND-MOST', 1), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(corpus[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved in data/doc2vec_input.csv\n",
      "Saved in data/doc2vec_labels.csv\n"
     ]
    }
   ],
   "source": [
    "save_vectors(model.docvecs.vectors_docs, 'doc2vec')\n",
    "save_labels('doc2vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try out new tsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bhtsne\n",
    "\n",
    "%time output = bhtsne.tsne(doc_bow_svd, dimensions=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 31s, sys: 5.3 s, total: 2min 36s\n",
      "Wall time: 2min 36s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=3)\n",
    "%time output_2 = tsne.fit_transform(doc_bow_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = np.loadtxt(\"data/tfidf_input.csv\", skiprows=1)\n",
    "# embedding_array = bhtsne.run_bh_tsne(doc_bow_svd, initial_dims=doc_bow_svd.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
